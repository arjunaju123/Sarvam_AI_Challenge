{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunaju123/Sarvam_AI_Challenge/blob/master/Copy_of_%5BSarvam_ai%5D_speech_team_Hiring_Challenge'24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸš€ **Join Our Speech Team at Sarvam AI!** ğŸš€\n",
        "\n",
        "\n",
        "\n",
        "We at [Sarvam AI](https://www.sarvam.ai/) are on a mission to revolutionize the GenAI landscape in India, and we're looking for talented **Data Engineers** and **ML Engineers** to help us achieve this. Become part of a team dedicated to building state-of-the-art ML systems for Speech Recognition and Text-to-Speech applications in Indian languages.\n",
        "\n",
        "\n",
        "\n",
        "**Why Join Our Speech Team?**\n",
        "\n",
        "- ğŸ§  **Develop Cutting-Edge ML Models:** Work on the forefront of machine learning to develop systems that understand and speak multiple Indian languages.\n",
        "\n",
        "- ğŸ“Š **Handle Large-Scale Data Sets:** Dive into dataset curation, managing and processing over 1M hours of audio data to train our models.\n",
        "\n",
        "- ğŸ’» **Advanced GPU Infrastructure:** Get hands-on experience with our massive A100 and H100 GPU cluster, pushing the boundaries of what AI can achieve.\n",
        "\n",
        "- ğŸ› ï¸ **Solve Real-World Problems:** Tackle hard applied research and engineering challenges with a direct impact on our products.\n",
        "\n",
        "- ğŸŒ **Work with Top Talent:** Collaborate with some of India's best ML Engineers and Product Developers in an environment that values innovation and creativity.\n",
        "\n",
        "\n",
        "\n",
        "**We're Excited to Offer the Following Opportunities (Full Time only):**\n",
        "\n",
        "- ğŸŒ± **Summer Internship (2 months on-site/remote, earning up to 50k per month):** Ideal for freshers or students with a foundational grasp of ML and programming.\n",
        "\n",
        "  - **Data Engineer:** Engage in web scraping, manage distributed data processing, and develop robust data pipelines.\n",
        "\n",
        "  - **ML Engineer:** Train, monitor and eval state-of-the-art speech models.\n",
        "\n",
        "  \n",
        "\n",
        "- ğŸ”¥ **AI Residency (6 months on-site, earning up to 1L per month):** Perfect for those with professional experience or significant expertise. Contribute to our groundbreaking projects and refine your skills.\n",
        "\n",
        "  - **Data Engineer:** Oversee large-scale data mining and sophisticated engineering tasks for massive data flows.\n",
        "\n",
        "  - **ML Engineer:** Train advanced speech models on powerful GPU clusters using frameworks like PyTorch and JAX, and tools like NeMo and HuggingFace Transformers.\n",
        "\n",
        "\n",
        "\n",
        "**About Sarvam AI:**\n",
        "\n",
        "Sarvam AI is a well-funded GenAI startup focused on creating full-stack GenAI systems and applications tailored for India. Based out of Bangalore and Chennai, Sarvam AI is a place for those who are passionate about driving significant advancements through Generative AI, have a love for Indian languages, and are eager to make a substantial impact.\n",
        "\n",
        "\n",
        "\n",
        "**Join Us!**\n",
        "\n",
        "Complete our [Hiring Challenge in the Colab Notebook](https://colab.research.google.com/drive/1EiiLTf5zB8Jm2PxdU3H20rWUr40FrsGM?usp=sharing) to showcase your skills! After completing the challenge, please apply through this [Google Form](https://forms.gle/yP4Vd9QhiETTqEtW8). Weâ€™re eager to see your innovative solutions and potentially welcome you aboard to help shape the future of AI in India.\n",
        "\n",
        "\n",
        "\n",
        "*Note: Admissions are on a rolling basis until all positions are filled. Apply early to secure your spot in our team!*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hzyClfgaJEbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "97jDW6dhYI0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸŒŸ Welcome to the Speech Team Hiring Challenge! ğŸš€\n",
        "\n",
        "Hey there! We're thrilled to kick off this exciting challenge with two awesome tasks tailored to test your prowess in speech and text data analysis. These tasks are crucial for our hiring process and mirror the real-world scenarios our team loves to tackle. ğŸ¯\n",
        "\n",
        "**Task 1: Semantic Chunking of a YouTube Video** ğŸ“¹\n",
        "- Dive into extracting meaningful audio-text pairs from a specific video. Show us your skill in achieving precise segmentation and alignment!\n",
        "\n",
        "**Task 2: Exploratory Data Analysis of New Testament Audio and Text** ğŸ“–\n",
        "- Get your hands dirty with a deep dive into the audio and text from the New Testament in your mother tongue. We're looking for sharp insights that could revolutionize text-to-speech and speech-to-text technologies.\n",
        "\n",
        "Please tackle both tasks with your full creativity and analytical skills as they are equally important in our evaluation. ğŸ¤“ ğŸ‹ï¸â€â™‚ï¸ Your innovative approaches, depth of analysis, and tech-savviness will be key to understanding how well you fit into our dynamic team.\n",
        "\n",
        "**Submission Instructions:**\n",
        "- Make sure to create a copy of your Google Colab notebook for each task.\n",
        "- Set the notebook to **shareable** and grant viewing access to **abhigyan@sarvam.ai**.\n",
        "- Once you've perfected your work, please paste the link to your notebook in the [Google Form provided](https://forms.gle/qxy2LF4Jtph7xhYHA). This step is crucial for us to review your submissions properly.\n",
        "\n",
        "Good luck, and let's see what amazing things you can uncover! ğŸŒˆğŸ‘€\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "upoYo6G_-5kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Semantic Chunking of a Youtube Video\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "The objective is to extract high-quality, meaningful (semantic) segments from the specified YouTube video: [Watch Video](https://www.youtube.com/watch?v=Sby1uJ_NFIY).\n",
        "\n",
        "Suggested workflow:\n",
        "1. **Download Video and Extract Audio:** Download the video and separate the audio component.\n",
        "2. **Transcription of Audio:** Utilize an open-source Speech-to-Text model to transcribe the audio. *Provide an explanation of the chosen model and any techniques used to enhance the quality of the transcription.*\n",
        "3. **Time-Align Transcript with Audio:** *Describe the methodology and steps for aligning the transcript with the audio.*\n",
        "4. **Semantic Chunking of Data:** Slice the data into audio-text pairs, using both semantic information from the text and voice activity information from the audio, with each audio-chunk being less than 15s in length. *Explain the logic used for semantic chunking and discuss the strengths and weaknesses of your approach.*\n",
        "\n",
        "**Judgement Criteria:**\n",
        "\n",
        "1. **Precision-Oriented Evaluation:** The evaluation focuses on precision rather than recall. Higher scores are achieved by reporting fewer but more accurate segments rather than a larger number of segments with inaccuracies. Segment accuracy is determined by:\n",
        "   - **Transcription Quality:** Accuracy of the text transcription for each audio chunk.\n",
        "   - **Segment Quality:** Semantic richness of the text segments.\n",
        "   - **Timestamp Accuracy:** Precision of the start and end times for each segment. Avoid audio cuts at the start or end of a segment.\n",
        "   \n",
        "2. **Detailed Explanations:** Provide reasoning behind each step in the process.\n",
        "3. **Generalization:** Discuss the general applicability of your approach, potential failure modes on different types of videos, and adaptation strategies for other languages.\n",
        "4. **[Bonus-1]** **Gradio-app Interface:** Wrap your code in a gradio-app which takes in youtube link as input and displays the output in a text-box.\n",
        "5. **[Bonus-2]** **Utilizing Ground-Truth Transcripts:** Propose a method to improve the quality of your transcript using a ground-truth transcript provided as a single text string. Explain your hypothesis for this approach. *Note that code-snippet isn't required for this question.*\n",
        "\n",
        "  As an example - for the audio extracted from [yt-link](https://www.youtube.com/watch?v=ysLiABvVos8), how can we leverage transcript scraped from [here](https://www.newsonair.gov.in/bulletins-detail/english-morning-news-7/), to improve the overall transcription quality of segments?\n",
        "\n",
        "**Submission Format:**\n",
        "\n",
        "Your submission should be a well-documented Jupyter notebook capable of reproducing your results. The notebook should automatically install all required dependencies and output the results in the specified format.\n",
        "\n",
        "- **Output Format:** Provide the results as a list of dictionaries, each representing a semantic chunk. Each dictionary should include:\n",
        "  - `chunk_id`: A unique identifier for the chunk (integer).\n",
        "  - `chunk_length`: The duration of the chunk in seconds (float).\n",
        "  - `text`: The transcribed text of the chunk (string).\n",
        "  - `start_time`: The start time of the chunk within the video (float).\n",
        "  - `end_time`: The end time of the chunk within the video (float).\n",
        "\n",
        "```python\n",
        "sample_output_list = [\n",
        "    {\n",
        "        \"chunk_id\": 1,\n",
        "        \"chunk_length\": 14.5,\n",
        "        \"text\": \"Here is an example of a semantic chunk from the video.\",\n",
        "        \"start_time\": 0.0,\n",
        "        \"end_time\": 14.5,\n",
        "    },\n",
        "    # Additional chunks follow...\n",
        "]\n",
        "```\n",
        "\n",
        "Ensure that your code is clear, well-commented, and easy to follow, with explanations for each major step and decision in the process. The notebook should be able to install all the dependencies automatically and generate the reported output when run.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mzqLZrA2UEmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W00nRD-WTTXR"
      },
      "outputs": [],
      "source": [
        "# Your code for Task 1 goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download Video and Extract Audio:\n",
        "Download the video and separate the audio component."
      ],
      "metadata": {
        "id": "Up3xAGl0Yz7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqveWP5GYkN2",
        "outputId": "b29b137e-211d-4d6e-e725-d3dbc08bb24d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[32m51.2/57.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# URL of the YouTube video you want to download\n",
        "video_url = \"https://www.youtube.com/watch?v=Sby1uJ_NFIY\"\n",
        "\n",
        "# Initialize a YouTube object with the video URL\n",
        "yt = YouTube(video_url)\n",
        "\n",
        "# Select the highest resolution stream available\n",
        "video_stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "# Download the video\n",
        "video_stream.download(output_path=\"/content/drive/MyDrive/Sarvam_AI_Challenge\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "cgz7glRuYkQb",
        "outputId": "f6e15b0f-3d57-447c-a9ff-342282699521"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Sarvam_AI_Challenge/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# URL of the YouTube video you want to download\n",
        "video_url = \"https://www.youtube.com/watch?v=Sby1uJ_NFIY\"\n",
        "\n",
        "# Initialize a YouTube object with the video URL\n",
        "yt = YouTube(video_url)\n",
        "\n",
        "# Get the audio stream of the video\n",
        "audio_stream = yt.streams.filter(only_audio=True).first()\n",
        "\n",
        "# Download the audio\n",
        "audio_stream.download(output_path=\"/content/drive/MyDrive/Sarvam_AI_Challenge\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "m7dM3S7JYkTH",
        "outputId": "d8c29c70-8d21-4043-e465-9bac82154027"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Sarvam_AI_Challenge/Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription of Audio: Utilize an open-source Speech-to-Text model to transcribe the audio. Provide an explanation of the chosen model and any techniques used to enhance the quality of the transcription."
      ],
      "metadata": {
        "id": "MowX0TTrbmpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q --upgrade pip\n",
        "!pip install --q --upgrade git+https://github.com/huggingface/transformers.git accelerate datasets[audio]\n",
        "!pip install --q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8ir9TrbYkVe",
        "outputId": "79348d16-4642-4866-e05d-8a3ff2b825ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
      ],
      "metadata": {
        "id": "cbnSIyidYkX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
      ],
      "metadata": {
        "id": "BwvDXY3GYkaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"openai/whisper-large-v3\""
      ],
      "metadata": {
        "id": "8LZxQW8AYkc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "dohQPE1gYkfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=15,\n",
        "    batch_size=16,\n",
        "    return_timestamps=True,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "jvKUuZEmYkh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe"
      ],
      "metadata": {
        "id": "mlv52KrIYkkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "result = pipe(audio) # v2 gives better results if you don't provide a language."
      ],
      "metadata": {
        "id": "-4_awatiYkm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(audio)"
      ],
      "metadata": {
        "id": "FSFnM3LSYkpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['text']"
      ],
      "metadata": {
        "id": "G-_FHPuiYksE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['chunks']"
      ],
      "metadata": {
        "id": "78ZTL_WrYkuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "result = pipe(audio, generate_kwargs={\"language\": \"english\"})"
      ],
      "metadata": {
        "id": "zVm0IUrDYkw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(audio, return_timestamps=True)\n",
        "print(result[\"chunks\"])\n"
      ],
      "metadata": {
        "id": "V0v2NRQxYkzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkgXqdWHYk2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTkWFRWuYk4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ejNCcJXYk7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPsZVJ7jYk-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bh5Wv3ktYlDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXT3YqirYlF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0yegDnGYlIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbexrBBLYlLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvjRFyUnYlNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSyKzWW2YlQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCGD5qtkYlTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQpTX5P9YlWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmxYIgGMYlbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQlD4y8vYlhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBCCxrXCYlmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QViqpTpWYlpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ucdeNYTnYlsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Rfhd2AVYlvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bdd3aczQYl08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrwBOAl5Yl30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhI_k3M9Yl66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppcuzMgmYl9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5gZghP8YmAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Q-76YmMYmDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBGwgkKEYmJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_bSUzqqYmL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AA84CY6UYmOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdrfYksiYmQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC-MHtnuYo6U",
        "outputId": "84deb811-1470-4482-8abd-a0854db544e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Exploratory Data Analysis of New Testament Audio and Text\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "The objective of this task is to conduct a comprehensive exploratory data analysis (EDA) on the audio and text data of the 260 chapters of the New Testament in your mother tongue (excluding English). The data should be obtained through web scraping from [Faith Comes By Hearing](https://www.faithcomesbyhearing.com/).\n",
        "\n",
        "The workflow for this task should include:\n",
        "1. **Web Scraping:** Systematically download the audio files and their corresponding textual content for each of the 260 chapters of the New Testament from the specified website.\n",
        "2. **Data Preparation:** Organize the data by chapters, ensuring each audio file is matched with its corresponding text.\n",
        "3. **Exploratory Data Analysis:** Analyze the data to uncover patterns, anomalies, or insights that could benefit applications such as Text to Speech (TTS) and Speech to Text (STT) technologies. Your analysis should explore various facets of the data, including audio quality, text clarity, and alignment between text and spoken content.\n",
        "\n",
        "**Judgement Criteria:**\n",
        "\n",
        "Your submission will be evaluated based on:\n",
        "- **Efficiency and Reliability of Web Scraping Techniques:** The methods and tools used for downloading the chapters efficiently and reliably.\n",
        "- **Data Analysis Methods:** The techniques and approaches used for analyzing the audio and text data.\n",
        "- **Quality of Data Analysis:** How effectively the analysis addresses potential applications for the Speech team, including TTS and STT technologies.\n",
        "- **Creativity in Analysis:** Innovative approaches in data handling and analysis, and the use of relevant metrics to assess data quality and applicability.\n",
        "\n",
        "**Submission Requirements:**\n",
        "\n",
        "Your submission should include the following components:\n",
        "- **Report on Key Performance Indicators (KPIs):** A concise report detailing the key findings from your analysis, focusing on aspects that are critical for improving TTS and STT applications.\n",
        "- **Methodological Explanation:** A thorough explanation of the methods used for both web scraping and the exploratory data analysis. This should include challenges faced and how they were overcome.\n",
        "- **Supporting Materials:** Include code snippets and visualizations that highlight significant insights from the data. These should be well-documented and easy to understand, demonstrating the logic behind your analytical decisions.\n",
        "\n",
        "The report should be structured to clearly present the methodology, findings, and implications of your analysis. It should be technical yet accessible, aimed at stakeholders who may have varying levels of familiarity with data analysis techniques.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xBt8cbX_WpQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for Task 2 goes here!"
      ],
      "metadata": {
        "id": "bq4Q0tszWrAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}